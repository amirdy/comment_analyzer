{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K33BOVDkVmEu",
        "outputId": "807dde9e-f5c9-439f-cd6f-e36413f46a20"
      },
      "source": [
        "import numpy as np \n",
        "import torch\n",
        "import os \n",
        "import nltk\n",
        "import random\n",
        "import torch.cuda as cuda\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices()) # Show the GPU & CPU Specifications\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3924550064263071265\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15505193728\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 323020358180229692\n",
            "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
            "]\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXtXY9azWSNw"
      },
      "source": [
        "### division ###\n",
        "\n",
        "train_samples_path = \"/content/drive/MyDrive/sentiment/train.ft.txt\"\n",
        "\n",
        "file = open(train_samples_path, \"r\")\n",
        "train = []\n",
        "for line in file.readlines():\n",
        "  train.append( line )\n",
        "file.close()\n",
        "\n",
        "random.shuffle(train)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/sentiment/train_data.txt\", \"w\") as output:\n",
        "    output.writelines(train[:780000])\n",
        "with open(\"/content/drive/MyDrive/sentiment/validation_data.txt\", \"w\") as output:\n",
        "    output.writelines(train[780000:1000000])    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOO8kOdJV9L5"
      },
      "source": [
        "train_samples_path = \"/content/drive/MyDrive/sentiment/train_data.txt\"\n",
        "validation_samples_path = \"/content/drive/MyDrive/sentiment/validation_data.txt\"\n",
        "test_samples_path = \"/content/drive/MyDrive/sentiment/test.ft.txt\"\n",
        "\n",
        "file = open(train_samples_path, \"r\")\n",
        "train_data = []\n",
        "train_label = []\n",
        "for line in file.readlines():\n",
        "  x = line[11:-1].lower().replace(\".\", \" . \")   \n",
        "  train_data.append( word_tokenize(x) )\n",
        "  train_label.append(int(line[9]) - 1)\n",
        "file.close()\n",
        "\n",
        "file = open(validation_samples_path, \"r\")\n",
        "validation_data = []\n",
        "validation_label = []\n",
        "for line in file.readlines():\n",
        "  x = line[11:-1].lower().replace(\".\", \" . \")   \n",
        "  validation_data.append( word_tokenize(x) )\n",
        "  validation_label.append(int(line[9]) - 1)\n",
        "file.close()\n",
        "\n",
        "file = open(test_samples_path, \"r\")\n",
        "test_data = []\n",
        "test_label = []\n",
        "for line in file.readlines():\n",
        "  x = line[11:-1].lower().replace(\".\", \" . \")   \n",
        "  test_data.append( word_tokenize(x) )\n",
        "  test_label.append(int(line[9]) - 1)\n",
        "file.close()\n",
        "\n",
        "#### Train data is 2D array and train label is 1D array\n",
        "\n",
        "print(\"Sample 0 of train data : {}\".format(train_data[0]))\n",
        "print(\"Sample 0 of train label : {}\".format(train_label[0]))\n",
        "\n",
        "#### Validation data is 2D array and train label is 1D array\n",
        "\n",
        "print(\"Sample 0 of validation data : {}\".format(validation_data[0]))\n",
        "print(\"Sample 0 of validation label : {}\".format(validation_label[0]))\n",
        "\n",
        "#### Test data is 2D array and train label is 1D array\n",
        "\n",
        "print(\"Sample 0 of test data : {}\".format(test_data[0]))\n",
        "print(\"Sample 0 of test label : {}\".format(test_label[0]), end = \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Lenght of train data is {}\".format(len(train_data)))\n",
        "print(\"Lenght of validation data is {}\".format(len(validation_data)))\n",
        "print(\"Lenght of test data is {}\".format(len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOQNCI-1a6YG"
      },
      "source": [
        "vocab = ['UNK', 'PAD']\n",
        "vocab_repeat = [0, 0] # Number of repetitions of vocabs\n",
        "\n",
        "### Creating Vocabulary from training data \n",
        "for sample in train_data:\n",
        "  for word in sample:\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "      vocab_repeat.append(1)\n",
        "    else:\n",
        "      index = vocab.index(word)\n",
        "      vocab_repeat[index] += 1   \n",
        "\n",
        "print('done')\n",
        "\n",
        "#### Vocabs with less than 10 repetitions will be replaced with UNK.\n",
        "Minimum_of_repetition = 10\n",
        "vocab_remove = []\n",
        "for index_sample, sample in enumerate(train_data):\n",
        "  # if index_sample % 50000 == 0:\n",
        "  #   print(index_sample)\n",
        "  for index_word, word in enumerate(sample):\n",
        "    if  vocab_repeat[vocab.index(word)] < Minimum_of_repetition:\n",
        "        train_data[index_sample][index_word] = 'UNK'\n",
        "        if word not in vocab_remove:\n",
        "          vocab_remove.append(word)\n",
        "\n",
        "for word in vocab_remove:\n",
        "  index = vocab.index(word)\n",
        "  vocab.remove(word)\n",
        "  del(vocab_repeat[index])\n",
        "\n",
        "print(\"The size of the vocabulary is : {}\".format(len(vocab)))\n",
        "print(\"The vocab is : \\n{}\\n\".format(vocab))\n",
        "print(\"The vocab repeat is : \\n{}\\n\".format(vocab_repeat))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7yJAr8Vg0S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de3f16a-8082-43c9-cab9-fdc1f73f706f"
      },
      "source": [
        "## Evaluate sizes - calculating MIN, MAX, MEANâ€Œ, STD\n",
        "mean_size_of_samples = 0\n",
        "std_of_samples = 0  \n",
        "max = 0\n",
        "min =100000\n",
        "\n",
        "for sample in train_data:\n",
        "  mean_size_of_samples += len(sample)\n",
        "  if len(sample) > max :\n",
        "    max = len(sample)\n",
        "  if len(sample) < min :\n",
        "    min = len(sample)\n",
        "\n",
        "mean_size_of_samples = mean_size_of_samples / len(train_data)\n",
        "\n",
        "for sample in train_data:\n",
        "  std_of_samples += (len(sample) - mean_size_of_samples)**2\n",
        "std_of_samples = np.sqrt( std_of_samples / len(train_data) )\n",
        "\n",
        "print(\"Train:  Max sample length is : {}\".format(max))\n",
        "print(\"Train: Min sample length is : {}\".format(min))\n",
        "print(\"Train: Mean sample length is : {}\".format(mean_size_of_samples))\n",
        "print(\"Train: Std of lenghts is : {}\".format(std_of_samples))\n",
        "\n",
        "sU = mean_size_of_samples + 3*std_of_samples\n",
        "\n",
        "print(\"Mean + 3 * Std  is : {}\".format(sU))\n",
        "\n",
        "\n",
        "### Check : How many train samples have lenght <= SU\n",
        "count = 0\n",
        "for sample in train_data:\n",
        "  if ( len(sample) <= int(sU)  ):\n",
        "    count += 1\n",
        "print(\"It is {}% of samples\".format(count/len(train_data) * 100 ))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:  Max sample length is : 501\n",
            "Train: Min sample length is : 12\n",
            "Train: Mean sample length is : 92.70222692307692\n",
            "Train: Std of lenghts is : 50.23249259158384\n",
            "Mean + 3 * Std  is : 243.39970469782844\n",
            "It is 99.96358974358974% of samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FviHS_uzhKoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae495d9e-a0cf-4076-b756-646eda7899d1"
      },
      "source": [
        "## ADD Padding and Cutting the long samples \n",
        "seq_lenght = int(sU) + 1\n",
        "print(\"The sequence lenght is {}\".format(seq_lenght))\n",
        "\n",
        "## If the size of sample is less than seq_lenght, we add enough <pad> to reach the size of seq_lenght. \n",
        "## Or if the size of sample is bigger than seq_lenght, we just use the first seq_lenght vocabs. \n",
        "\n",
        "for idx_sample, sample in enumerate(train_data):\n",
        "  if len(sample) < seq_lenght:\n",
        "    for i in range( seq_lenght -len(sample) ):\n",
        "      train_data[idx_sample].append('PAD')\n",
        "  else :\n",
        "      train_data[idx_sample] = train_data[idx_sample][0:seq_lenght]\n",
        "\n",
        "for idx_sample, sample in enumerate(validation_data):\n",
        "  if len(sample) < seq_lenght:\n",
        "    for i in range( seq_lenght -len(sample) ):\n",
        "      validation_data[idx_sample].append('PAD')\n",
        "  else :\n",
        "      validation_data[idx_sample] = validation_data[idx_sample][0:seq_lenght]\n",
        "\n",
        "for idx_sample, sample in enumerate(test_data):\n",
        "  if len(sample) < seq_lenght:\n",
        "    for i in range( seq_lenght -len(sample) ):\n",
        "      test_data[idx_sample].append('PAD')\n",
        "  else :\n",
        "      test_data[idx_sample] = test_data[idx_sample][0:seq_lenght]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sequence lenght is 244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6hoQ0nzYAdq"
      },
      "source": [
        "## Convert Vocabs to their indexes in Vocab. \n",
        "\n",
        "for idx_sample, sample in enumerate(train_data):\n",
        "  for idx_word, word in enumerate(sample):\n",
        "    train_data[idx_sample][idx_word] = vocab.index(word)    \n",
        "\n",
        "\n",
        "for idx_sample, sample in enumerate(validation_data):\n",
        "  for idx_word, word in enumerate(sample):\n",
        "    if word not in vocab:\n",
        "      validation_data[idx_sample][idx_word] = 0 ## UNK for vocabs that are not in the Vocab.\n",
        "    else:\n",
        "      validation_data[idx_sample][idx_word] = vocab.index(word)    \n",
        "\n",
        "\n",
        "for idx_sample, sample in enumerate(test_data):\n",
        "  for idx_word, word in enumerate(sample):\n",
        "    if word not in vocab:\n",
        "      test_data[idx_sample][idx_word] = 0 ## UNK for vocabs that are not in the Vocab.\n",
        "    else:\n",
        "      test_data[idx_sample][idx_word] = vocab.index(word)    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hiZqwLVs3ie"
      },
      "source": [
        "#### Train data is 2D array and Train label is 1D array\n",
        "print(\"Sample 0 of train data : {}\".format(train_data[0]))\n",
        "print(\"Sample 0 of train label : {}\".format(train_label[0]))\n",
        "\n",
        "#### Validation data is 2D array and Validation label is 1D array\n",
        "\n",
        "print(\"Sample 0 of validation data : {}\".format(validation_data[0]))\n",
        "print(\"Sample 0 of validation label : {}\".format(validation_label[0]))\n",
        "\n",
        "#### Test data is 2D array and Test label is 1D array\n",
        "\n",
        "print(\"Sample 0 of test data : {}\".format(test_data[0]))\n",
        "print(\"Sample 0 of test label : {}\".format(test_label[0]), end = \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb-yCYMRe_v7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be307d4-4297-43f2-c03e-a42be93b8fb2"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/sentiment/vocab_new.txt\", \"wb\") as fp:   \n",
        "   pickle.dump(vocab, fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/sentiment/train_data_indexed_new.txt\", \"wb\") as fp:   \n",
        "   pickle.dump(train_data, fp)\n",
        "with open(\"/content/drive/MyDrive/sentiment/validation_data_indexed_new.txt\", \"wb\") as fp:   \n",
        "   pickle.dump(validation_data, fp)\n",
        "with open(\"/content/drive/MyDrive/sentiment/test_data_indexed_new.txt\", \"wb\") as fp:   \n",
        "   pickle.dump(test_data, fp)\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/sentiment/train_lebel_new.txt\", \"wb\") as fp:   \n",
        "   pickle.dump(train_label, fp)\n",
        "with open(\"/content/drive/MyDrive/sentiment/validation_label_new.txt\", \"wb\") as fp:  \n",
        "   pickle.dump(validation_label, fp)\n",
        "with open(\"/content/drive/MyDrive/sentiment/test_label_new.txt\", \"wb\") as fp:  \n",
        "   pickle.dump(test_label, fp)\n",
        "\n",
        "print(\"Done !!!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLGFa3pWcdb7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}